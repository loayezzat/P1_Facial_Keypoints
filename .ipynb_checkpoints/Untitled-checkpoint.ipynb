{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: define the convolutional neural network architecture\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# can use the below import should you choose to initialize the weights of your Net\n",
    "import torch.nn.init as I\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        ## TODO: Define all the layers of this CNN, the only requirements are:\n",
    "        ## 1. This network takes in a square (same width and height), grayscale image as input\n",
    "        ## 2. It ends with a linear layer that represents the keypoints\n",
    "        ## it's suggested that you make this last layer output 136 values, 2 for each of the 68 keypoint (x, y) pairs\n",
    "        \n",
    "        # As an example, you've been given a convolutional layer, which you may (but don't have to) change:\n",
    "        # 1 input image channel (grayscale), 32 output channels/feature maps, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3 , padding = 1 )\n",
    "        self.batch1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding = 1 )\n",
    "        self.batch2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1 )\n",
    "        self.batch3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding = 1 )\n",
    "        self.batch4 = nn.BatchNorm2d(256)\n",
    "        self.pool =  nn.MaxPool2d(2, 2)    \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 *14*14,1024)\n",
    "        self.fc2 = nn.Linear(1024,136)\n",
    "        \n",
    "        \n",
    "        ## Note that among the layers to add, consider including:\n",
    "        # maxpooling layers, multiple conv layers, fully-connected layers, and other layers (such as dropout or batch normalization) to avoid overfitting\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TODO: Define the feedforward behavior of this model\n",
    "        ## x is the input image and, as an example, here you may choose to include a pool/conv step:\n",
    "        batch_size = x.shape[0]\n",
    "        print(\"Input to conv1 \",x.shape)\n",
    "        x = self.pool(self.batch1(F.relu(self.conv1(x))))\n",
    "        print(\"Input to conv2 \",x.shape)\n",
    "        x = self.pool(self.batch2(F.relu(self.conv2(x))))\n",
    "        print(\"Input to conv3 \",x.shape)\n",
    "        x = self.pool(self.batch3(F.relu(self.conv3(x))))\n",
    "        print(\"Input to conv4 \",x.shape)\n",
    "        x = self.pool(self.batch4(F.relu(self.conv4(x))))\n",
    "        print(\"before flatten \",x.shape)\n",
    "        x = x.view (batch_size, -1)\n",
    "        print(\"After flatten , input to fc1\",x.shape)\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        print(\"input to fc2 \",x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"output \",x.shape)\n",
    "        # a modified x, having gone through all the layers of your model, should be returned\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(*[10, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to conv1  torch.Size([10, 1, 224, 224])\n",
      "Input to conv2  torch.Size([10, 32, 112, 112])\n",
      "Input to conv3  torch.Size([10, 64, 56, 56])\n",
      "Input to conv4  torch.Size([10, 128, 28, 28])\n",
      "before flatten  torch.Size([10, 256, 14, 14])\n",
      "After flatten , input to fc1 torch.Size([10, 50176])\n",
      "input to fc2  torch.Size([10, 1024])\n",
      "output  torch.Size([10, 136])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0857, -0.5807,  0.5919,  ...,  0.2178,  0.3461, -0.3465],\n",
       "        [-0.1529, -0.2156,  0.8035,  ...,  0.0771, -1.3335, -0.1825],\n",
       "        [ 0.2094,  0.4499,  0.5781,  ...,  0.0585, -1.0505, -0.2053],\n",
       "        ...,\n",
       "        [ 0.1552,  0.1554, -0.1582,  ..., -0.1263, -1.8063,  0.1292],\n",
       "        [-0.4397,  0.2299,  0.3702,  ..., -0.2403, -0.0991,  0.8300],\n",
       "        [ 0.2098,  0.3990, -0.7370,  ..., -0.1352,  0.9483,  0.7281]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
